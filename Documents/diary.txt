2018-02-20

The first step in the project is to extract the feature from the dataset and prepare it for SVM analysis. In order to do this, a code was written which converts the text file with protein sequences into a more convenient format -  a dictionary, where the keys are the IDs of protein sequences, and the values are lists with first items being the sequences and the second items - the features.

2018-02-21

The input vector for SVM should be two lists: one containing all the features and one all the labels, as shown in the example below:


e.g. A window of three amino acids, considering there are only 5 amino acids in total. Label is for the second position.

X = [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0]

y = [1]


In order to achieve this notation, I first did the following steps:

1. Wrote a code which takes in a text file with protein sequences and returns a set with unique amino acids present. This was to check how many amino acids there are in my dataset which will be useful when building a dictionary and creating binary encodings for each of the amino acids (code: amino_acid_count.py). The answer turned out to be 20, and the set was: {'S', 'N', 'R', 'M', 'F', 'H', 'D', 'W', 'Q', 'I', '\n', 'A', 'V', 'G', 'Y', 'P', 'K', 'C', 'L', 'T', 'E'}.

2. Wrote a code which contains amino acid dictionary (keys are amino acids, denoted by letters, and values are lists of 20 binary numbers, unique for each amino acid) and a function which, when given a protein sequence, consults this dictionary, and returns a list containing sublists with binary encodings for each amino acid. So for example, an input sequence 'AC' would return [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]] (code: amino_acid_encodings.py).

This outcome is good if window size is only one amino acid. However, we want the code to also take window size as input, and return a list with sublists, where each sublist contains encodings for amino acids in that window. So for example, if the window size is one, the outcome would be exactly the same as earlier but if it is 3, the sublist would contain three times as many numbers. I then did the following:

3. Wrote a function which takes a sequence and window size as inputs and returns a list of windows (code: window_list.py). So for example, an input of 'ACWQEVR' and window size 2 would return this list ['AC', 'CW', 'WQ', 'QE', 'EV', 'VR'] and if the window size was 5, this list ['ACWQE', 'CWQEV', 'WQEVR'].

4. Wrote a function, which takes this list of windows and, after consulting the dictionary, creates a big list containing sublists (nested lists), each representing a window, and containing encodings for each amino acid in the sequence. An input of ['AC', 'CD'] would return [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]. Code: encodings_window_size.py

5. Merged function in steps 3 and 4 to create a function which takes a sequence and window size as inputs and turns into a form compatible with SVM input.
An input of 'ACD' and window size 1 would return: [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
An input of 'ACD' and window size 3 would return: [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
Code: encodings_final.py

By using this function, it should be possible to take the protein sequences from the values in the dataset dictionary and turn them into this form, compatible with training on SVM. Each of the nested lists should also have a corresponding label (0 or 1, which denote exposed or burried) in a different list. If the nested list only contains encodings for one amino acid, it should correspond to the state of that amino acid; if it contains encodings for a window of aminoacids, it should correspond to the state of the central amino acid.

6. Wrote a function which takes a sequence (with E and B as letters) and window size as inputs, converts it to a list of windows (using the window_list function I wrote earlier) and uses this list to take the central residue of each window (unless it's 1, then it just takes the item as it is) and makes a new list in which this residue is denoted by a binary label (1 or 0). So an input sequence of 'EBBEBEB' and window size 3 would return: [1, 1, 0, 1, 0] because the window list would be ['EBB', 'BBE', 'BEB', 'EBE', 'BEB'], and the same input sequence with window size 1 would return [0, 1, 1, 0, 1, 0, 1] because the list would be ['E', 'B', 'B', 'E', 'B', 'E', 'B']. Code: labels.py

By doing these steps, I now have two major functions: one which takes a protein sequence and based on window size turns it into lists of features with one-hot encodings (X=[[1,0,0,....],[0,1,0...],...]) (code: encodings_final.py) and one which takes a label sequence (e.g.'EEBBEB') and based on window size turns it into a list of binary classes for each central residue in a window (Y-[0,1,0,0...]) (code: labels.py). I now want to merge them into one function which takes a list in a form of ['SEQUENCE','EEBBEBBE'] and based on window size returns two lists: X and Y. Then this function can be used to iterate over all the values in the dataset dictionary and merge all of these smaller X and Y lists into big X and Y lists containing information about the entire dataset, which can be used as input for SVM.

The merged function can be found in code: two_lists.py. If an input is ['ACDEQY','EBBEBE'] with the window size 1, the output is ([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]], [0, 1, 1, 0, 1, 0]).

The final function which takes a dictionary and window size as inputs, iterates over dictionary values and returns two big lists, can be found in two_lists_iteration.py. If the input dictionary is my_dict={'ID1':['ACD','EBB'],'ID2':['QRA','BBE']} and the window size is 1, the function will return: [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [0, 1, 1, 1, 1, 0]).

2018-02-22

Looked for relevant publications for my project (exposed and buried residues in membrane proteins).

1.  Helms, V., Hayat, S., & Metzger, J. (2010). Predicting the burial/exposure status of transmembrane residues in helical membrane proteins. In Structural Bioinformatics of Membrane Proteins (pp. 151-164). Springer, Vienna.

2. Park, Y., Hayat, S., & Helms, V. (2007). Prediction of the burial status of transmembrane residues of helical membrane proteins. BMC bioinformatics, 8(1), 302.

3. Wang, C., Li, S., Xi, L., Liu, H., & Yao, X. (2011). Accurate prediction of the burial status of transmembrane residues of α-helix membrane protein by incorporating the structural and physicochemical features. Amino acids, 40(3), 991-1002.

4. Illergård, K., Callegari, S., & Elofsson, A. (2010). MPRAP: An accessibility predictor for a-helical transmem-brane proteins that performs well inside and outside the membrane. BMC bioinformatics, 11(1), 333.

5. Yuan, Z., Zhang, F., Davis, M. J., Bodén, M., & Teasdale, R. D. (2006). Predicting the solvent accessibility of transmembrane residues from protein sequence. Journal of proteome research, 5(5), 1063-1070.

The first article reviews the different prediction methods and seems to give a general overview of the field which might be useful for the project. The second and third article describe state-of-art burial status prediction methods. The last two articles describe methods which predict the burial state indirectly, by evaluating the solvent accessibility to the residue.

2018-02-23

Attended the journal club for deep learning and wrote a plan for the project.

2018-02-26

Read the article “Predicting the burial/exposure status of transmembrane residues in helical membrane proteins” as a preparation for the oral presentation and worked on a code which creates the SVM_input (SVM_input.py). The problem of first and last residues when creating window list still remains and will have to be tackled tomorrow.

2018-02-27

The problem with creating a list of all the possible windows is the fact that features of the last residues are not taken into account. In order to correct for that, I wrote a code which adds a certain number of letters B, which do not stand for any amino acid and would be encoded by zeros in the dictionary. The number of Bs depends on the window size and was defined as int(wind_sz/2) (e.g. if window size is 7, the number of Bs added to each end of the sequence would be 3). The input sequence ‘AGH’ with window size 3 would be subsequently transformed into ‘BAGHB’ and would yield a window list ['BAG', 'AGH', 'GHB’]. Code can be found in updated_window_list.py. This new feature was added to the function which iterates over a dictionary of sequences and labels and creates two lists suitable for SVM input (encoded amino acids and labels) (Code: input_updated.py). In this function, the part which creates a list of encoded labels subsequently does not need to be as complicated as before. Since the new list of amino acid windows takes into account all the residues, it all the labels will be taken into account. Thus there is no need to create a window list first and is enough to just create a list of labels (encoded by 0 or 1) (a short trial code can be found in label_encodings.py).

Worked on a function which does the training on one dataset, performs prediction on another dataset and counts the number of correct predictions (SVM_input2.py).

2018-02-28

Prepared for the oral presentation of an article.

2018-02-29

Listened to oral presentations of four other people (Kyle, Kajetan, Daryl and Sharmishtaa) as well as presented my own. Wrote evaluations for the presentations.

2018-03-02

A program which takes in a sequence and feature and creates an input vector for SVM is in the folder Codes/2018-02-27 and is called SVM_input2.py. The two datasets used in the code are in the Datasets folder, however the names of the files can be specified in the first section of the code.

2018-03-05

Today I was reading some literature on burial status prediction methods to gain more insight into the field and prepare for the presentation. Helped me get the general overview of the available methods (Beuming and Weinstein, Yuan, TMX and the novel Wang et al method). 



