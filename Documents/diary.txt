2018-02-20

The first step in the project is to extract the feature from the dataset and prepare it for SVM analysis. In order to do this, a code was written which converts the text file with protein sequences into a more convenient format -  a dictionary, where the keys are the IDs of protein sequences, and the values are lists with first items being the sequences and the second items - the features.

2018-02-21

The input vector for SVM should be two lists: one containing all the features and one all the labels, as shown in the example below:


e.g. A window of three amino acids, considering there are only 5 amino acids in total. Label is for the second position.

X = [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0]

y = [1]


In order to achieve this notation, I first did the following steps:

1. Wrote a code which takes in a text file with protein sequences and returns a set with unique amino acids present. This was to check how many amino acids there are in my dataset which will be useful when building a dictionary and creating binary encodings for each of the amino acids (code: amino_acid_count.py). The answer turned out to be 20, and the set was: {'S', 'N', 'R', 'M', 'F', 'H', 'D', 'W', 'Q', 'I', '\n', 'A', 'V', 'G', 'Y', 'P', 'K', 'C', 'L', 'T', 'E'}.

2. Wrote a code which contains amino acid dictionary (keys are amino acids, denoted by letters, and values are lists of 20 binary numbers, unique for each amino acid) and a function which, when given a protein sequence, consults this dictionary, and returns a list containing sublists with binary encodings for each amino acid. So for example, an input sequence 'AC' would return [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]] (code: amino_acid_encodings.py).

This outcome is good if window size is only one amino acid. However, we want the code to also take window size as input, and return a list with sublists, where each sublist contains encodings for amino acids in that window. So for example, if the window size is one, the outcome would be exactly the same as earlier but if it is 3, the sublist would contain three times as many numbers. I then did the following:

3. Wrote a function which takes a sequence and window size as inputs and returns a list of windows (code: window_list.py). So for example, an input of 'ACWQEVR' and window size 2 would return this list ['AC', 'CW', 'WQ', 'QE', 'EV', 'VR'] and if the window size was 5, this list ['ACWQE', 'CWQEV', 'WQEVR'].

4. Wrote a function, which takes this list of windows and, after consulting the dictionary, creates a big list containing sublists (nested lists), each representing a window, and containing encodings for each amino acid in the sequence. An input of ['AC', 'CD'] would return [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]. Code: encodings_window_size.py

5. Merged function in steps 3 and 4 to create a function which takes a sequence and window size as inputs and turns into a form compatible with SVM input.
An input of 'ACD' and window size 1 would return: [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
An input of 'ACD' and window size 3 would return: [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
Code: encodings_final.py

By using this function, it should be possible to take the protein sequences from the values in the dataset dictionary and turn them into this form, compatible with training on SVM. Each of the nested lists should also have a corresponding label (0 or 1, which denote exposed or burried) in a different list. If the nested list only contains encodings for one amino acid, it should correspond to the state of that amino acid; if it contains encodings for a window of aminoacids, it should correspond to the state of the central amino acid.

6. Wrote a function which takes a sequence (with E and B as letters) and window size as inputs, converts it to a list of windows (using the window_list function I wrote earlier) and uses this list to take the central residue of each window (unless it's 1, then it just takes the item as it is) and makes a new list in which this residue is denoted by a binary label (1 or 0). So an input sequence of 'EBBEBEB' and window size 3 would return: [1, 1, 0, 1, 0] because the window list would be ['EBB', 'BBE', 'BEB', 'EBE', 'BEB'], and the same input sequence with window size 1 would return [0, 1, 1, 0, 1, 0, 1] because the list would be ['E', 'B', 'B', 'E', 'B', 'E', 'B']. Code: labels.py

By doing these steps, I now have two major functions: one which takes a protein sequence and based on window size turns it into lists of features with one-hot encodings (X=[[1,0,0,....],[0,1,0...],...]) (code: encodings_final.py) and one which takes a label sequence (e.g.'EEBBEB') and based on window size turns it into a list of binary classes for each central residue in a window (Y-[0,1,0,0...]) (code: labels.py). I now want to merge them into one function which takes a list in a form of ['SEQUENCE','EEBBEBBE'] and based on window size returns two lists: X and Y. Then this function can be used to iterate over all the values in the dataset dictionary and merge all of these smaller X and Y lists into big X and Y lists containing information about the entire dataset, which can be used as input for SVM.

The merged function can be found in code: two_lists.py. If an input is ['ACDEQY','EBBEBE'] with the window size 1, the output is ([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]], [0, 1, 1, 0, 1, 0]).

The final function which takes a dictionary and window size as inputs, iterates over dictionary values and returns two big lists, can be found in two_lists_iteration.py. If the input dictionary is my_dict={'ID1':['ACD','EBB'],'ID2':['QRA','BBE']} and the window size is 1, the function will return: [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [0, 1, 1, 1, 1, 0]).

2018-02-22

Looked for relevant publications for my project (exposed and buried residues in membrane proteins).

1.  Helms, V., Hayat, S., & Metzger, J. (2010). Predicting the burial/exposure status of transmembrane residues in helical membrane proteins. In Structural Bioinformatics of Membrane Proteins (pp. 151-164). Springer, Vienna.

2. Park, Y., Hayat, S., & Helms, V. (2007). Prediction of the burial status of transmembrane residues of helical membrane proteins. BMC bioinformatics, 8(1), 302.

3. Wang, C., Li, S., Xi, L., Liu, H., & Yao, X. (2011). Accurate prediction of the burial status of transmembrane residues of α-helix membrane protein by incorporating the structural and physicochemical features. Amino acids, 40(3), 991-1002.

4. Illergård, K., Callegari, S., & Elofsson, A. (2010). MPRAP: An accessibility predictor for a-helical transmem-brane proteins that performs well inside and outside the membrane. BMC bioinformatics, 11(1), 333.

5. Yuan, Z., Zhang, F., Davis, M. J., Bodén, M., & Teasdale, R. D. (2006). Predicting the solvent accessibility of transmembrane residues from protein sequence. Journal of proteome research, 5(5), 1063-1070.

The first article reviews the different prediction methods and seems to give a general overview of the field which might be useful for the project. The second and third article describe state-of-art burial status prediction methods. The last two articles describe methods which predict the burial state indirectly, by evaluating the solvent accessibility to the residue.

2018-02-23

Attended the journal club for deep learning and wrote a plan for the project.

2018-02-26

Read the article “Predicting the burial/exposure status of transmembrane residues in helical membrane proteins” as a preparation for the oral presentation and worked on a code which creates the SVM_input (SVM_input.py). The problem of first and last residues when creating window list still remains and will have to be tackled tomorrow.

2018-02-27

The problem with creating a list of all the possible windows is the fact that features of the last residues are not taken into account. In order to correct for that, I wrote a code which adds a certain number of letters B, which do not stand for any amino acid and would be encoded by zeros in the dictionary. The number of Bs depends on the window size and was defined as int(wind_sz/2) (e.g. if window size is 7, the number of Bs added to each end of the sequence would be 3). The input sequence ‘AGH’ with window size 3 would be subsequently transformed into ‘BAGHB’ and would yield a window list ['BAG', 'AGH', 'GHB’]. Code can be found in updated_window_list.py. This new feature was added to the function which iterates over a dictionary of sequences and labels and creates two lists suitable for SVM input (encoded amino acids and labels) (Code: input_updated.py). In this function, the part which creates a list of encoded labels subsequently does not need to be as complicated as before. Since the new list of amino acid windows takes into account all the residues, it all the labels will be taken into account. Thus there is no need to create a window list first and is enough to just create a list of labels (encoded by 0 or 1) (a short trial code can be found in label_encodings.py).

Worked on a function which does the training on one dataset, performs prediction on another dataset and counts the number of correct predictions (SVM_input2.py).

2018-02-28

Prepared for the oral presentation of an article.

2018-02-29

Listened to oral presentations of four other people (Kyle, Kajetan, Daryl and Sharmishtaa) as well as presented my own. Wrote evaluations for the presentations.

2018-03-02

A program which takes in a sequence and feature and creates an input vector for SVM is in the folder Codes/2018-02-27 and is called SVM_input2.py. The two datasets used in the code are in the Datasets folder, however the names of the files can be specified in the first section of the code.

2018-03-05

Today I was reading some literature on burial status prediction methods to gain more insight into the field and prepare for the presentation. Helped me get the general overview of the available methods (Beuming and Weinstein, Yuan, TMX and the novel Wang et al method). 

2018-03-06

Continuation on literature studies and final touches on the presentation based on the feedback I received during evaluations.

2018-03-08

After the presentation, continued working with the code:

1. Wrote a code which does cross-validation on a dataset and returns an average cross-validation score (Cross_val.py). To do: run this program on the complete dataset with different window sizes as inputs and record average score for each input. 

Information about cross validation:

The cross validation score by default is the accuracy of the prediction, although it can be set to measure other things.

Changing the window size and calculating the cross validation score each time to choose the best window size is one of the ways to optimize the model. Another method would be to change other parameters in the SVC function, such as gamma, kernel type. The kernel can be linear, radial, etc. Gamma is the kernel coefficient which configures the sensitivity to differences in feature values. If you set it too high, you risk overfitting.

2. Next, I need to write a code which takes in the full dataset as an input, creates a model, takes a dataset of other random proteins, parses it, predicts their features and transforms the predicted output in a specific form which should look like this:
>ID
Seq
Topology 

Wrote this function, it is stored in: Model_and_prediction.py
The datasets used can be found in folder Datasets.

2018-03-12

I decided to reorganize my code to make it more structured.

The model builder and predictor is now split into two separate files:

1. Model_builder.py

Builds a model and saves it using the Pickle package.

2. Predictor.py

Loads the saved model using the Pickle package and uses it to predict the feature for sequences in FASTA format.

Also, the following codes were written:

3. Cross_val.py

Checks different window sizes and returns cross validation scores for each window size.

Tried running it on the full file of 230 sequences but it took a very long time just to get 3 different window sizes tested, so I decided to run it on small but still representative dataset of 50 proteins.

4. Grid_search.py (unfinished)

Trying out different parameters to optimize the model. 

2018-03-13

Wrote a code Cross_val_parameters.py which runs the cross validation on linear SVC with different C-scores and window sizes. The highest prediction accuracy was obtained at window size 19 and C-score of 0.3. These parameters were selected to build the model with the training dataset (70% of the original dataset).

The built model was then tested on a testing dataset (remaining 30% of the full dataset) and the accuracy of the prediction was measured. (Code: Prediction_accuracy.py)

2018-03-14

Updated Prediction_accuracy.py code to calculate the confusion matrix, accuracy, sensitivity and Matthew’s correlation coefficient. For linear SVC with parameters C score = 0.3 and window size 19, the following were obtained:

Confusion matrix: 
 [[10114  3241]
 [ 3578  6685]]
Accuracy (precision): 
 0.7386795208881098
Sensitivity (recall): 
 0.7573193560464245
Matthew's correlation coefficient: 
 0.41042214848077496

Wrote a code which creates an SVM input from PSSMs generated by PSI-BLAST (PSSM_parser.py)

2018-03-15

PSSM_parser.py now can build a model based on input generated from PSSM data.

Saved a model with parameters of c-score 0.3 and window size 19. Ran the prediction on testing_dataset.txt. Accuracy report:

Confusion matrix: 
 [[10237  3118]
 [ 6401  3862]]
Accuracy (precision): 
 0.6152782786392595
Sensitivity (recall): 
 0.7665293897416698
Matthew's correlation coefficient: 
 0.15517016662493655

The accuracy using PSSM model with the same parameters is worse than with regular model. Optimization needs to be done by cross validation.

--

I realized that I used my PSSM model on input from testing dataset which was not in a format that PSSM model was trained on. The X_array needs to be in the format of PSSM too.

In order to fix that, I will create FASTA files for my testing dataset proteins, run PSIBLAST to retrieve PSSM profiles and create SVM inputs from those.

--

Ran prediction on testing_dataset.txt, this time using input in the required format, using a model with parameters: c-score=0.3 and window size=19. Accuracy report:

Confusion matrix: 
 [[10344  3011]
 [ 2889  7374]]
Accuracy (precision): 
 0.7816821582407617
Sensitivity (recall): 
 0.7745413702733058
Matthew's correlation coefficient: 
 0.49239431117444005

---

Cross validation on the training dataset with input to SVM in PSSM format resulted in the best cross validation score of 0.7559044982 for c-score 0.1 and window size 13. These were used to build a model. Ran prediction on testing_dataset.txt in the required format, using the model with these optimized parameters. Accuracy report:

Confusion matrix: 
 [[10327  3028]
 [ 2879  7384]]
Accuracy (precision): 
 0.781993033469635
Sensitivity (recall): 
 0.7732684387869712
Matthew's correlation coefficient: 
 0.49195983317451913

---

Updated my predictor so it takes in an example sequence, based on its name finds PSSM profile in a folder and creates an X array (used for SVM input) based on that PSSM profile and uses the before built model to predict the topology. (Predictor_PSSM.py)

To do:
- Plot the cross validation scores of regular (done) and PSSM models (Optimal parameters regular model: c-score 0.3 window size 19, PSSM model c-score 0.1 window size 13)
- Make tables comparing the accuracy of optimized regular and PSSM models at predicting;
- Write a READ_ME file where I explain where the model and the predictor is;
- Write the report
